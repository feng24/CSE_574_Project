# -*- coding: utf-8 -*-
"""Eas503_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A1XonhqyhOIjFpih893lu1zQdy6hpEoX

# setting and preparing
"""

!pip install kaggle

from google.colab import files
files.upload()
!ls -lha kaggle.json

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d ronitf/heart-disease-uci

!unzip heart-disease-uci.zip

"""#explore data"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('heart.csv')
df.head()
df.info()

"""the data is pretty cleannow, see the correlation"""

df.corr()

"""use matplotlib and seaborn to visualize data"""

plt.figure(figsize=(16,8))
ax = sns.heatmap(df.corr(), linewidth = 0.5,annot = True)
plt.show()

"""cp,restecg,thalach, and slope have positive correlation, among themï¼Œcp and thalach are more related to target

# histogram and samples' distribution

age: The person's age in years
sex: The person's sex (1 = male, 0 = female)
cp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)
trestbps: The person's resting blood pressure (mm Hg on admission to the hospital)
chol: The person's cholesterol measurement in mg/dl
fbs: The person's fasting blood sugar (> 120 mg/dl, 1 = true; 0 = false)
restecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)
thalach: The person's maximum heart rate achieved
exang: Exercise induced angina (1 = yes; 0 = no)
oldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)
slope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)
ca: The number of major vessels (0-4)
thal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)
target: Heart disease (0 = no, 1 = yes)
"""

df.hist(figsize=(20,15))
plt.show()

"""#scatterplot
Create a scatterplot to visually assess the nature of an association between two
continuous variables

focus on :cp, thalach, slope and restecg
"""

sns.scatterplot(x=df['thalach'],y=df['cp'],hue=df['target'])
plt.xlabel('thalach')
plt.ylabel('cp')
plt.show()

"""at the same level of cp, a higher thalach(maximum heart rate), a more frequnt heart disease



at the same level of thalach, a higher cp, a more frequent heart disease
"""

df_train = df[['cp','thalach','slope','restecg']]
df_train.head(10)

df_compare_train = df.drop('target',axis=1)
df_compare_train.head()

df_lable = df[['target']]
df_lable.head()

"""Split arrays  into random train and test subsets"""

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test = train_test_split(df_train,df_lable, test_size = 0.4)

x_compare_train,x_compare_test,y_compare_train,y_compare_test = train_test_split(df_compare_train,df_lable, test_size = 0.4)

y_train = y_train.values.reshape(-1,1)
y_test = y_test.values.reshape(-1,1)

y_compare_train = y_compare_train.values.reshape(-1,1)
y_compare_test = y_compare_test.values.reshape(-1,1)

from sklearn import linear_model
from sklearn import metrics

Rid = linear_model.RidgeClassifier()
Rid.fit(x_train,y_train.ravel())

Rid_compare = linear_model.RidgeClassifier()
Rid_compare.fit(x_compare_train,y_compare_train.ravel())

RidPredict = Rid.predict(x_test)
Rid_compare_predict = Rid_compare.predict(x_compare_test)

print(Rid.score(x_train,y_train))
print('MAE:',metrics.mean_squared_error(y_test, RidPredict))
print(Rid_compare.score(x_compare_train,y_compare_train))
print('MAE:',metrics.mean_squared_error(y_compare_test, Rid_compare_predict))

plt.figure(figsize=(16,8))
plt.plot(y_compare_test,label = 'True Data')
plt.plot(Rid_compare_predict,label = 'Predict Data')
plt.xlabel('Times')
plt.ylabel('Data')
plt.title('Accurancy of Prediction')
plt.show()

from sklearn import svm

SVM = svm.SVC()
SVM.fit(x_train,y_train.ravel())

SVM_compare = svm.SVC()
SVM_compare.fit(x_compare_train,y_compare_train.ravel())

SVM_Predict = SVM.predict(x_test)
SVM_compare_predict = SVM_compare.predict(x_compare_test)
print(SVM.score(x_train,y_train))
print('MAE:',metrics.mean_squared_error(y_test, SVM_Predict))
print(SVM_compare.score(x_compare_train,y_compare_train))
print('MAE:',metrics.mean_squared_error(y_compare_test, SVM_compare_predict))

plt.figure(figsize=(16,8))
plt.plot(y_test,label = 'True Data')
plt.plot(SVM_Predict,label = 'Predict Data')
plt.xlabel('Times')
plt.ylabel('Data')
plt.title('Accurancy of Prediction')
plt.show()

from sklearn.ensemble import RandomForestClassifier

Randomtree = RandomForestClassifier(n_estimators=80,max_depth=80)
Randomtree.fit(x_train,y_train.ravel())

Randomtree_compare = RandomForestClassifier(n_estimators=150,max_depth=10)
Randomtree_compare.fit(x_compare_train,y_compare_train.ravel())

Randomtree_predict = Randomtree.predict(x_test)
Randomtree_compare_predict = Randomtree_compare.predict(x_compare_test)

print(Randomtree.score(x_train,y_train))
print('MAE:',metrics.mean_squared_error(y_test, Randomtree_predict))
print(Randomtree_compare.score(x_compare_train,y_compare_train))
print('MAE:',metrics.mean_squared_error(y_compare_test, Randomtree_predict))

plt.figure(figsize=(16,8))
plt.plot(y_test,label = 'True Data')
plt.plot(Randomtree_predict,label = 'Predict Data')
plt.xlabel('Times')
plt.ylabel('Data')
plt.title('Accurancy of Prediction')
plt.show()

from sklearn.ensemble import GradientBoostingClassifier

GBC = GradientBoostingClassifier()
GBC.fit(x_train,y_train.ravel())

GBC_compare = GradientBoostingClassifier()
GBC_compare.fit(x_compare_train,y_compare_train.ravel())

GBC_predict = GBC.predict(x_test)
GBC_compare_predict = GBC_compare.predict(x_compare_test)

print(GBC.score(x_train,y_train))
print('MAE:',metrics.mean_squared_error(y_test, GBC_predict))
print(GBC_compare.score(x_compare_train,y_compare_train))
print('MAE:',metrics.mean_squared_error(y_compare_test, GBC_compare_predict))

plt.figure(figsize=(16,8))
plt.plot(y_compare_test,label = 'True Data')
plt.plot(GBC_compare_predict,label = 'Predict Data')
plt.xlabel('Times')
plt.ylabel('Data')
plt.title('Accurancy of Prediction')
plt.show()

"""*   All: Ridge GradientBooost
*   part of them: SVM RandomForest
"""

import numpy as np
labels = ('RidgeClassifier','SVM','RandomForest','GrandientBoost')
errors = (metrics.mean_squared_error(y_compare_test, Rid_compare_predict),metrics.mean_squared_error(y_test, SVM_Predict),
          metrics.mean_squared_error(y_test, Randomtree_predict),metrics.mean_squared_error(y_compare_test, GBC_compare_predict))

plt.figure(figsize=(8,8))
x = np.arange(len(labels))

plt.bar(x,errors,color='blue',label='Error')
plt.ylabel('Errors')
plt.xticks(x,labels)
plt.legend()

plt.tight_layout()
plt.show()

import numpy as np
labels = ('RidgeClassifier','SVM','RandomForest','GrandientBoost')
errors = (Rid_compare.score(x_compare_train,y_compare_train),SVM.score(x_train,y_train),Randomtree_compare.score(x_compare_train,y_compare_train),GBC_compare.score(x_compare_train,y_compare_train))

plt.figure(figsize=(8,8))
x = np.arange(len(labels))

plt.bar(x,errors,color='gray',label='performance')
plt.ylabel('Accuracy')
plt.xticks(x,labels)
plt.legend()

plt.tight_layout()
plt.show()